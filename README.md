# Desaf칤os de procesamiento del lenguaje natural

Especializaci칩n en IA - UBA (Universidad Nacional de Buenos Aires) - Projectos finales

### Desaf칤o 1:

En el primer desaf칤o, se abordaron los fundamentos de la vectorizaci칩n de texto, utilizando posteriormente un modelo de clasificaci칩n como Na칦ve Bayes para encontrar similitudes entre textos. Para ello, se emple칩 el conjunto de datos _20 Newsgroups_ de scikit-learn.

Para la vectorizaci칩n se siguieron los siguientes pasos:
1. Se instanci칩 un vectorizador.
2. Se entren칩 el vectorizador.

Para valorar la similitud entre texto se uso la similitud del coseno.

Luego se optimizaron estos modelo para lograr el maximo valor de la metrica F1-score. El mejor valor obtenido fue de 0.77.

Por ultimo se invirtio la matriz documento-t칠rmino para estudiar similitudes entre palabras. En la siguiente figura se puede ver un ejemplo de predicciones de palabras similares:

![Predicciones de palabras](desafio1_similitud_palabras.png)

El codigo se encuentra en [Desaf칤o 1](https://github.com/FrancoArtale/Procesamiento_lenguaje_natural/blob/main/Desafio_1.ipynb).

Enlaces de utilidad:
- https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html
- https://www.geeksforgeeks.org/cosine-similarity/
- https://en.wikipedia.org/wiki/Document-term_matrix

### Desaf칤o 2:

A partir de un dataset (corpus), se crearon vectores de palabras (embeddings) utilizando la biblioteca Gensim. Los vectores fueron graficados para observar las similitudes en el espacio de embeddings. Finalmente, se plantearon pruebas de analog칤as.

El dataset utilizado fue la Biblia del proyecto Gutenberg, la cual fue preprocesada antes de crear los vectores. El preprocesamiento incluy칩 los siguientes pasos:
1. Lematizaci칩n.
2. Eliminaci칩n de los signos de puntuaci칩n.
3. Filtrado las _stropwords_.
Para generar los vectores, se utiliz칩 un modelo Word2Vec. Posteriormente, el modelo generador de vectores fue entrenado durante 100 칠pocas.

En la siguiente figura, se pueden ver resultados similares usando la similitud coseno de la palabra _god_:

![image](https://github.com/user-attachments/assets/1dbfa956-efd8-409d-bd85-27ddeae0a70c)

En la siguiente figura, se pueden observar los resultados menos similares de la palabra _prayer_:

![image2](https://github.com/user-attachments/assets/09d24eab-a47d-448b-9269-7df7e7b2af5d)

En la siguiente imagen, se muestra la distribuci칩n de los vectores en el espacio de embeddings:

![image3](https://github.com/user-attachments/assets/d8153e05-3012-4bfa-8026-c942529704c7)

En el gr치fico, podemos ver que las palabras "David" y "altar" est치n muy cerca entre s칤. Seg칰n la Biblia, David construy칩 un altar e hizo sacrificios all칤, lo que podr칤a explicar esta relaci칩n que se observa en el gr치fico.

En el gr치fico tambi칠n se puede apreciar una cercan칤a entre "seven" y "priest" (sacerdote). Esto tal vez se deba a los siete sacramentos, de los cuales la mayor칤a solo puede ser administrada por un sacerdote.

Asimismo, "water" y "sea" est치n cercanos.

Uno de los tests de analog칤as que se realiz칩 fue: "orador" es a "Dios" como "sacrificio" es a "altar", con el fin de obtener palabras relacionadas con "altar". Sin embargo, se obtuvieron resultados m치s cercanos a "sacrificio". En la siguiente imagen se puede ver lo obtenido:

![image](https://github.com/user-attachments/assets/f00edb48-5c59-4b07-b0bf-a5c62438300e)

El codigo se encuentra en [Desaf칤o 2](https://github.com/FrancoArtale/Procesamiento_lenguaje_natural/blob/main/Desafio_2.ipynb).

Enlaces de utilidad:
- https://radimrehurek.com/gensim/
- https://www.baeldung.com/cs/latent-vs-embedding-space
- https://www.gutenberg.org/
- https://www.datacamp.com/es/tutorial/stemming-lemmatization-python
- https://blackbeast.pro/diccionario/stop-words/#:~:text=Las%20Stop%20Words%20o%20palabras,la%20palabra%20clave%20o%20keyword.

### Desaf칤o 3:

Se entrenaron modelos de lenguaje con tokenizaci칩n por caracteres y por palabras. Estos modelos se utilizaron para la generaci칩n de secuencias. Para la generaci칩n, se emplearon tanto la predicci칩n m치s probable como m칠todos como Beam Search y muestreo aleatorio para armar las secuencias.

El dataset (corpus) utilizado en este caso fue el libro Viaje al centro de la Tierra de Julio Verne, descargado del proyecto Gutenberg.

Preprocesamiento de datos:
1. Tokenizaci칩n del libro.
2. Divisi칩n de las sentencias en segmentos de 100 caracteres o palabras, seg칰n el modelo.
3. Separaci칩n en entrenamiento y validaci칩n.

Para el entrenamiento, se utiliz칩 la m칠trica de perplejidad. Las arquitecturas utilizadas fueron RNN, LSTM y GRU, todas entrenadas durante 20 칠pocas.

En la siguiente figura, se muestra el rendimiento de las distintas arquitecturas en el caso de tokenizaci칩n por caracteres:

![image](https://github.com/user-attachments/assets/1a90ba5e-6eab-4418-95dc-ba02cc20ba57)

En la siguiente figura, se muestra el rendimiento de las distintas arquitecturas en el caso de tokenizaci칩n por palabras:

![image](https://github.com/user-attachments/assets/c1c120da-4961-4b7e-9531-48f06f8a3236)

En la siguiente figura se puede ver las predicciones de las distintas arquitecturas en el caso de tokenizaci칩n por caracteres usando Beam Search estoc치stico:

![image](https://github.com/user-attachments/assets/dfc5c4dc-2485-4292-a3fa-113424b11e98)


En la siguiente figura, se muestran las predicciones de las distintas arquitecturas en el caso de tokenizaci칩n por palabras utilizando Beam Search estoc치stico.

![image](https://github.com/user-attachments/assets/43aefdf7-8628-427c-92ec-a88364aa1bae)

El codigo se encuentra en [Desaf칤o 3](https://github.com/FrancoArtale/Procesamiento_lenguaje_natural/blob/main/Desafio%203/desafio_3.ipynb).

Enlaces de utilidad:
- https://www.gutenberg.org/
- https://charanhu.medium.com/unveiling-the-power-of-character-level-tokenization-in-natural-language-processing-b2adadbbc72c#:~:text=Character%2Dlevel%20tokenization%20operates%20by,text%20into%20its%20atomic%20components.
- https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17
- https://njoroge.tomorrow.co.ke/blog/ai/word_vs_character_level_tokenization
- https://blog.dinobrain.ai/beam-search/#:~:text=El%20'beam%20search'%20es%20un,secuencias%20m%C3%A1s%20coherentes%20y%20precisas.
- https://klu.ai/glossary/perplexity
- https://keras.io/api/layers/recurrent_layers/
- https://keras.io/api/layers/recurrent_layers/lstm/
- https://keras.io/api/layers/recurrent_layers/gru/

### Desaf칤o 4:

En este desaf칤o, se lleva a cabo la implementaci칩n de un bot de preguntas y respuestas (QA). Se utiliza datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en ingl칠s. Los datos se pueden descargar desde [challenge ConvAI2 ](http://convai.io/data/).

Prepocesamiento de datos:
1. Se eliminaron los ap칩strofes mediante una funci칩n personalizada.
2. Las sentencias se limitaron a un m치ximo de 10 palabras.
3. Se tokenizaron las sentencias.
4. Se aplic칩 padding para uniformar la longitud de las sentencias.

Se utilizaron los embeddings de FastText para transformar los tokens de entrada en vectores, espec칤ficamente los embeddings _crawl-300d-2M.vec_.

El primer modelo entrenado se bas칩 en una arquitectura encoder-decoder, cada uno con una capa de embedding y una LSTM.

Los resultados del entrenamiento de este modelo se pueden observar en la siguiente figura:

![image](https://github.com/user-attachments/assets/c62995ab-2b26-4212-b32d-fb5efb8bc599)

En las siguientes im치genes, se pueden observar ejemplos de preguntas y respuestas generadas usando este modelo:

![image](https://github.com/user-attachments/assets/b617ac31-65f0-4647-961a-b93007d48814)
![image](https://github.com/user-attachments/assets/e0b5962d-ba99-482b-9c5b-39aaa94e231a)

El segundo modelo utilizado fue similar al anterior, pero con un stack de LSTMs: 3 capas para el encoder y 3 capas para el decoder, cada uno con sus respectivas capas de embeddings. Los resultados del entrenamiento se pueden ver en la siguiente figura:

![image](https://github.com/user-attachments/assets/4f65b331-52ea-40e5-aaaf-15c046f4bf15)

En las siguientes im치genes, se pueden observar ejemplos de preguntas y respuestas generadas usando este modelo:

![image](https://github.com/user-attachments/assets/afed733e-fa0a-40e6-84fa-965bc1d20d15)
![image](https://github.com/user-attachments/assets/e0b5962d-ba99-482b-9c5b-39aaa94e231a)

En el 칰ltimo modelo, se utiliz칩 una capa LSTM para el encoder y otra para el decoder, con sus correspondientes capas de embeddings y una capa de attention. Los resultados del entrenamiento se pueden ver en la siguiente figura:

![image](https://github.com/user-attachments/assets/28726f9c-234f-4426-9bb2-ffd493009dce)

En las siguientes im치genes, se pueden observar ejemplos de preguntas y respuestas generadas usando este modelo:

![image](https://github.com/user-attachments/assets/953e1b17-d191-4c70-83ed-3e0ecbd53429)
![image](https://github.com/user-attachments/assets/9e90da13-4f91-4b41-b5a5-98df27837f7d)

El codigo se encuentra en [Desaf칤o 4](https://github.com/FrancoArtale/Procesamiento_lenguaje_natural/blob/main/Desafio_4/Desafio_4.ipynb).

Enlaces de utilidad:
- https://medium.com/@prashantmalge181/building-q-a-chat-bot-gemma-llm-with-hugging-face-233ddcc76fe8
- https://www.linkedin.com/advice/0/what-best-nlp-models-question-answering-skills-machine-learning-rzbgf
- https://medium.com/@mrmohit254/end-to-end-implementation-of-a-qa-bot-using-deep-learning-a3225e62bf22
- https://convai.io/data/
- https://fasttext.cc/docs/en/crawl-vectors.html
- https://keras.io/api/layers/recurrent_layers/lstm/
- https://medium.com/analytics-vidhya/https-medium-com-understanding-attention-mechanism-natural-language-processing-9744ab6aed6a
- https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html

## Muchas gracias!
Sientete libre de contactarte por mail a francoa23@gmail.com por cualquier duda.
Disfruta 游땏!!
